{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTEBOOK: Fine-tune a BERT-Tiny Model\n",
    "\n",
    "This notebook evaluates, simulates Envise-conditions, and fine-tunes a BERT-Tiny model for Envise-behavior using the Idiom Software Stack and the Accuracy Estimator tool. \n",
    "\n",
    "We are using the BERT \"Question-Answer\" model: [mrm8488/bert-tiny-finetuned-squadv2](https://huggingface.co/mrm8488/bert-tiny-finetuned-squadv2). It is relatively small and we can complete all the steps within 30 minutes. You can find the model details on Hugging Face.\n",
    "\n",
    "The dataset for this model is ``squadv2``.\n",
    "\n",
    "Run this Jupyter notebook on an environment that has a GPU instance.\n",
    "\n",
    "This notebook is in continuation to the `bert-tiny-profile.ipynb` notebook. To understand the full context, you run it before continuing with this notebook.\n",
    "\n",
    "**PROCESS**\n",
    "\n",
    "The following process describes the fine-tuning process used in this notebook:\n",
    "\n",
    "* First, evaluate the out-of-box accuracy of the FP32 BERT-Tiny model.\n",
    "\n",
    "* Next, estimate the model's accuracy in Envise precision by running the model using the Idiom API that calls the Envise Accuracy Estimator. Note that this is an estimate and most probably will be lower than the original FP32 accuracy.\n",
    "\n",
    "* Last, we apply the fine-tuning algorithm on the model by calling the Idiom API that returns a \"fine-tuned\" FP32 model with an improved accuracy score (closer to the original FP32 accuracy score). \n",
    "\n",
    "**SYSTEM COMPONENT MINIMUM REQUIREMENTS**\n",
    "\n",
    "* CPU: Any X86-64 architecture with 4 cores\n",
    "* RAM: 64 GB memory\n",
    "* GPU: One Nvidia 2080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from dataclasses import field\n",
    "from typing import Optional\n",
    "import torch\n",
    "from trainer_qa import QuestionAnsweringTrainer\n",
    "from transformers import HfArgumentParser\n",
    "from transformers import TrainingArguments\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up Idiom Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from idiom.ml.torch import setup_for_evaluation\n",
    "from idiom.ml.torch import setup_for_export\n",
    "from idiom.ml.torch import setup_for_tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up Imports from support scripts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_args import ModelArguments, IdiomMLArguments, DataTrainingArguments\n",
    "from trainer_support import get_trainer_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "check_min_version(\"4.16.2\")\n",
    "\n",
    "require_version(\n",
    "    \"datasets>=1.8.0\",\n",
    "    \"To fix: pip install -r examples/pytorch/question-answering/requirements.txt\",\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT fine-tuning and evaluation is parameterized with many argments, most of them use their default values. \n",
    "\n",
    "The arguments that are tailored for this notebook are available the the JSON file, ``tiny-bert-args.json``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "parser = HfArgumentParser(\n",
    "    (\n",
    "        ModelArguments,\n",
    "        IdiomMLArguments,\n",
    "        DataTrainingArguments,\n",
    "        TrainingArguments,\n",
    "    )\n",
    ")\n",
    "(\n",
    "    model_args,\n",
    "    idiom_ml_args,\n",
    "    data_args,\n",
    "    training_args,\n",
    ") = parser.parse_json_file(json_file=\"tiny-bert-args.json\")\n",
    "# added\n",
    "training_args._n_gpu = 1  # force this\n",
    "print(f'{model_args}, \\n{idiom_ml_args}, \\n{data_args}, \\n{training_args}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook needs support functions such as model and tokenizer download, training and evaluation datasets, and pre- and post-processing functions. \n",
    "All of these support functions have been placed outside the notebook.\n",
    "\n",
    "We just fetch them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params, all_datasets, other_params \\\n",
    "    = get_trainer_support(model_args, idiom_ml_args, data_args, training_args, logger)\n",
    "\n",
    "trainer, model = model_params['trainer'], model_params['model']\n",
    "eval_dataset, train_dataset  = all_datasets['eval_dataset'], all_datasets['train_dataset']\n",
    "eval_examples = all_datasets['eval_examples']\n",
    "tokenizer, data_collator = other_params['tokenizer'], other_params['data_collator']\n",
    "post_processing_function  = other_params['post_processing_function']\n",
    "compute_metrics = other_params['compute_metrics']\n",
    "last_checkpoint = other_params['last_checkpoint']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we evaluate the model as we received it, i.e., with FP32 precision.\n",
    "\n",
    "The accuracy metrics we track is `eval_exact`, the Exact Match. \n",
    "\n",
    "Exact Match is a match/no-match  measure of whether the evaluation output matches the\n",
    "ground truth answer exactly. This is a strict metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate out-of-the-box accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out of the Box Model Evaluation\n",
    "if training_args.do_eval:\n",
    "    oob_metrics = trainer.evaluate()\n",
    "    trainer.log_metrics(\"Out of the Box eval matrics\", oob_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we estimate the accuracy on Envise. Note, this is an estimate using Envise precision and the metric `eval_ecact` is expected to be lower that the original FP32 accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate for Envise Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Envise Evaluation\n",
    "if idiom_ml_args.do_envise_eval:\n",
    "    setup_for_evaluation(model)\n",
    "    \n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Envise Eval ***\")\n",
    "    metrics = trainer.evaluate()\n",
    "\n",
    "    max_eval_samples = (\n",
    "        data_args.max_eval_samples\n",
    "        if data_args.max_eval_samples is not None\n",
    "        else len(eval_dataset)\n",
    "    )\n",
    "    metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_args.do_train and idiom_ml_args.finetune_with_dft:\n",
    "    # prepare model to simulate training on Envise with DFT\n",
    "    trainer_dft = QuestionAnsweringTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset if training_args.do_train else None,\n",
    "        eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "        eval_examples=eval_examples if training_args.do_eval else None,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        post_process_function=post_processing_function,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    train_dft_dataloader = trainer_dft.get_train_dataloader()\n",
    "    inputs_dnf = next(iter(train_dft_dataloader))\n",
    "\n",
    "    def batch_process_func(model, inputs):\n",
    "        device = next(model.parameters()).device\n",
    "        for k in inputs:\n",
    "            inputs[k] = inputs[k].to(device)\n",
    "        with torch.no_grad():\n",
    "            model(**inputs)\n",
    "\n",
    "    setup_for_tuning(\n",
    "        model,\n",
    "        inputs=inputs_dnf,\n",
    "        batch_process_func=batch_process_func,\n",
    "    )\n",
    "    del trainer_dft\n",
    "\n",
    "# fine-tuning\n",
    "if training_args.do_train:\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    elif last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "\n",
    "    metrics = train_result.metrics\n",
    "    max_train_samples = (\n",
    "        data_args.max_train_samples\n",
    "        if data_args.max_train_samples is not None\n",
    "        else len(train_dataset)\n",
    "    )\n",
    "    metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulate Evaluation after Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if idiom_ml_args.do_envise_eval:\n",
    "    setup_for_evaluation(model)\n",
    "\n",
    "# Evaluation\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "    metrics = trainer.evaluate()\n",
    "\n",
    "    max_eval_samples = (\n",
    "        data_args.max_eval_samples\n",
    "        if data_args.max_eval_samples is not None\n",
    "        else len(eval_dataset)\n",
    "    )\n",
    "    metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if idiom_ml_args.setup_for_export:\n",
    "    model = setup_for_export(model)\n",
    "    model.save_pretrained(\"bert-tiny-idiom-finetuned\")\n",
    "    tokenizer.save_pretrained(\"bert-tiny-idiom-finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m transformers.onnx --model=bert-tiny-idiom-finetuned --feature=question-answering bert-tiny-idiom-onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "This tutorial shows how to use the Accuracy Estimator to simulate Envise conditions that evaluate the accuracy scores of a BERT-Tiny model.\n",
    "\n",
    "The results could vary slightly but are expected to stay in the range below:\n",
    "\n",
    "\n",
    "|   Eval type\t    |   EM score\t|\n",
    "|-------------------|---------------|\n",
    "|Out of the Box     |  48.5977  \t|\n",
    "|Envise w/no-tuning |  46.8037      |\n",
    "|One-epoch tuning   |  48.4629      |   \n",
    "\n",
    "\n",
    "If you fine-tune for more epochs, you could get better scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "90eb5e0fcbf302e2fbb7d64d0f5d2fef12836215bc1f384012a2fbc863f866e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
