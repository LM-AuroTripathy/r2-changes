{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a1162ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NOTEBOOK: Profile a BERT-Tiny Model\n",
    "\n",
    "This tutorial validates, compiles, and profiles a Bert-Tiny model for inference on Envise using the Idiom Software Stack.\n",
    "\n",
    "The model we're going to be working with is, mrm8488/bert-tiny-finetuned-squadv2. You can find more about this model on Hugging Face.\n",
    "\n",
    "Run this Jupyter notebook on an environment that has a GPU instance. \n",
    "\n",
    "The model will traverse through the following stages in the developer flow:\n",
    "\n",
    "**Export model to ONNX**\n",
    "    \n",
    "    The original model is exported to ONNX before validating operator coverage. \n",
    "\n",
    "**Validate the model**\n",
    "    \n",
    "    The operator coverage tool is invoked at this stage and checks for supported and unsupported ONXX operators in the model. \n",
    "\n",
    "**Compile the model** \n",
    "    \n",
    "    The compile() Idiom API is invoked at this stage and the ONNX model is compiled for Envise.\n",
    "\n",
    "**Profile the model** \n",
    "    \n",
    "    The profile() Idiom API is invoked at this stage and the model is executed at runtime in an Envise-simulated environment for performance metrics.\n",
    "\n",
    "**SYSTEM COMPONENT MINIMUM REQUIREMENTS**\n",
    "\n",
    "* CPU: Any X86-64 architecture with 4 cores\n",
    "* RAM: 64 GB memory\n",
    "* GPU: One Nvidia 2080"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d3954e-1478-4464-b71e-650fbd744718",
   "metadata": {},
   "source": [
    "#### Install Dependencies\n",
    "This step takes under one minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a197f48-4541-4f8a-80db-e699fbf0c0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in /opt/venv/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (5.9.1)\n",
      "Requirement already satisfied: accelerate==0.5.1 in /opt/venv/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (0.5.1)\n",
      "Requirement already satisfied: datasets==1.18.3 in /opt/venv/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.18.3)\n",
      "Requirement already satisfied: transformers[onnx]==4.16.2 in /opt/venv/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (4.16.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/venv/lib/python3.8/site-packages (from accelerate==0.5.1->-r requirements.txt (line 2)) (1.23.2)\n",
      "Requirement already satisfied: pyyaml in /opt/venv/lib/python3.8/site-packages (from accelerate==0.5.1->-r requirements.txt (line 2)) (6.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in /opt/venv/lib/python3.8/site-packages (from accelerate==0.5.1->-r requirements.txt (line 2)) (1.10.0+cu111)\n",
      "Requirement already satisfied: xxhash in /opt/venv/lib/python3.8/site-packages (from datasets==1.18.3->-r requirements.txt (line 3)) (3.0.0)\n",
      "Requirement already satisfied: pandas in /opt/venv/lib/python3.8/site-packages (from datasets==1.18.3->-r requirements.txt (line 3)) (1.4.3)\n",
      "Requirement already satisfied: aiohttp in /opt/venv/lib/python3.8/site-packages (from datasets==1.18.3->-r requirements.txt (line 3)) (3.8.1)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /opt/venv/lib/python3.8/site-packages (from datasets==1.18.3->-r requirements.txt (line 3)) (9.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/venv/lib/python3.8/site-packages (from datasets==1.18.3->-r requirements.txt (line 3)) (2022.7.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/venv/lib/python3.8/site-packages (from datasets==1.18.3->-r requirements.txt (line 3)) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/venv/lib/python3.8/site-packages (from datasets==1.18.3->-r requirements.txt (line 3)) (0.8.1)\n",
      "Requirement already satisfied: dill in /opt/venv/lib/python3.8/site-packages (from datasets==1.18.3->-r requirements.txt (line 3)) (0.3.5.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/venv/lib/python3.8/site-packages (from datasets==1.18.3->-r requirements.txt (line 3)) (2.28.1)\n",
      "Requirement already satisfied: packaging in /opt/venv/lib/python3.8/site-packages (from datasets==1.18.3->-r requirements.txt (line 3)) (21.3)\n",
      "Requirement already satisfied: multiprocess in /opt/venv/lib/python3.8/site-packages (from datasets==1.18.3->-r requirements.txt (line 3)) (0.70.13)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/venv/lib/python3.8/site-packages (from transformers[onnx]==4.16.2->-r requirements.txt (line 4)) (2022.7.25)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /opt/venv/lib/python3.8/site-packages (from transformers[onnx]==4.16.2->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: sacremoses in /opt/venv/lib/python3.8/site-packages (from transformers[onnx]==4.16.2->-r requirements.txt (line 4)) (0.0.53)\n",
      "Requirement already satisfied: filelock in /opt/venv/lib/python3.8/site-packages (from transformers[onnx]==4.16.2->-r requirements.txt (line 4)) (3.8.0)\n",
      "Requirement already satisfied: tf2onnx in /opt/venv/lib/python3.8/site-packages (from transformers[onnx]==4.16.2->-r requirements.txt (line 4)) (1.12.0)\n",
      "Requirement already satisfied: onnxruntime-tools>=1.4.2 in /opt/venv/lib/python3.8/site-packages (from transformers[onnx]==4.16.2->-r requirements.txt (line 4)) (1.7.0)\n",
      "Requirement already satisfied: onnxconverter-common in /opt/venv/lib/python3.8/site-packages (from transformers[onnx]==4.16.2->-r requirements.txt (line 4)) (1.12.1)\n",
      "Requirement already satisfied: onnxruntime>=1.4.0 in /opt/venv/lib/python3.8/site-packages (from transformers[onnx]==4.16.2->-r requirements.txt (line 4)) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/venv/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.18.3->-r requirements.txt (line 3)) (4.3.0)\n",
      "Requirement already satisfied: protobuf in /opt/venv/lib/python3.8/site-packages (from onnxruntime>=1.4.0->transformers[onnx]==4.16.2->-r requirements.txt (line 4)) (3.20.1)\n",
      "Requirement already satisfied: sympy in /opt/venv/lib/python3.8/site-packages (from onnxruntime>=1.4.0->transformers[onnx]==4.16.2->-r requirements.txt (line 4)) (1.10.1)\n",
      "Requirement already satisfied: coloredlogs in /opt/venv/lib/python3.8/site-packages (from onnxruntime>=1.4.0->transformers[onnx]==4.16.2->-r requirements.txt (line 4)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/venv/lib/python3.8/site-packages (from onnxruntime>=1.4.0->transformers[onnx]==4.16.2->-r requirements.txt (line 4)) (1.12)\n",
      "Requirement already satisfied: onnx in /opt/venv/lib/python3.8/site-packages (from onnxruntime-tools>=1.4.2->transformers[onnx]==4.16.2->-r requirements.txt (line 4)) (1.12.0)\n",
      "Requirement already satisfied: py3nvml in /opt/venv/lib/python3.8/site-packages (from onnxruntime-tools>=1.4.2->transformers[onnx]==4.16.2->-r requirements.txt (line 4)) (0.2.7)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/venv/lib/python3.8/site-packages (from onnxruntime-tools>=1.4.2->transformers[onnx]==4.16.2->-r requirements.txt (line 4)) (8.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/venv/lib/python3.8/site-packages (from packaging->datasets==1.18.3->-r requirements.txt (line 3)) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/venv/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.18.3->-r requirements.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/venv/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.18.3->-r requirements.txt (line 3)) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.18.3->-r requirements.txt (line 3)) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/venv/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.18.3->-r requirements.txt (line 3)) (3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/venv/lib/python3.8/site-packages (from aiohttp->datasets==1.18.3->-r requirements.txt (line 3)) (1.8.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/venv/lib/python3.8/site-packages (from aiohttp->datasets==1.18.3->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/venv/lib/python3.8/site-packages (from aiohttp->datasets==1.18.3->-r requirements.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/venv/lib/python3.8/site-packages (from aiohttp->datasets==1.18.3->-r requirements.txt (line 3)) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/venv/lib/python3.8/site-packages (from aiohttp->datasets==1.18.3->-r requirements.txt (line 3)) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/venv/lib/python3.8/site-packages (from aiohttp->datasets==1.18.3->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/venv/lib/python3.8/site-packages (from pandas->datasets==1.18.3->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/venv/lib/python3.8/site-packages (from pandas->datasets==1.18.3->-r requirements.txt (line 3)) (2022.2.1)\n",
      "Requirement already satisfied: six in /opt/venv/lib/python3.8/site-packages (from sacremoses->transformers[onnx]==4.16.2->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/venv/lib/python3.8/site-packages (from sacremoses->transformers[onnx]==4.16.2->-r requirements.txt (line 4)) (8.1.3)\n",
      "Requirement already satisfied: joblib in /opt/venv/lib/python3.8/site-packages (from sacremoses->transformers[onnx]==4.16.2->-r requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/venv/lib/python3.8/site-packages (from coloredlogs->onnxruntime>=1.4.0->transformers[onnx]==4.16.2->-r requirements.txt (line 4)) (10.0)\n",
      "Requirement already satisfied: xmltodict in /opt/venv/lib/python3.8/site-packages (from py3nvml->onnxruntime-tools>=1.4.2->transformers[onnx]==4.16.2->-r requirements.txt (line 4)) (0.13.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/venv/lib/python3.8/site-packages (from sympy->onnxruntime>=1.4.0->transformers[onnx]==4.16.2->-r requirements.txt (line 4)) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a96e5-fbb1-4265-a8c2-3ec94dda953e",
   "metadata": {},
   "source": [
    "#### Set up Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e7b8f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Mapping\n",
    "from collections import OrderedDict\n",
    "\n",
    "# HuggingFace imports\n",
    "import datasets\n",
    "from transformers.onnx.convert import export\n",
    "from transformers.onnx.config import OnnxConfig\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "\n",
    "# Lightmatter imports\n",
    "import idiom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf75e42-de47-4a53-8c8b-2a856d076016",
   "metadata": {},
   "source": [
    "#### Define Inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b7f2d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OnnxConfig is an abstract class, so we need a concrete base class that\n",
    "# provides a name for each tensor & their dimensions. These names are\n",
    "# emitted into the ONNX file.\n",
    "class BertOnnxConfig(OnnxConfig):\n",
    "    def __init__(self, config, task):\n",
    "        super().__init__(config,task)\n",
    "\n",
    "    @property\n",
    "    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n",
    "        return OrderedDict(\n",
    "            {\n",
    "                \"input_ids\":      {0: \"batch\", 1: \"sequence\"},\n",
    "                \"attention_mask\": {0: \"batch\", 1: \"sequence\"},\n",
    "                \"token_type_ids\": {0: \"batch\", 1: \"sequence\"}\n",
    "            }\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def outputs(self) -> Mapping[str, Mapping[int, str]]:\n",
    "        return OrderedDict(\n",
    "            {\n",
    "                \"start_logits\": {0: \"batch\", 1: \"sequence\"},\n",
    "                \"end_logits\":   {0: \"batch\", 1: \"sequence\"}\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aaa63d-9414-4f4b-b152-c7d1cfaba7fa",
   "metadata": {},
   "source": [
    "#### Define a Function to Encode Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "291a26c8-ea3d-4062-accb-28ea4da21036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_batch(tokenizer, questions, contexts, seq_len):\n",
    "    '''\n",
    "    Calls tokenizer.encode_plus() for each given question+context pair. All\n",
    "    samples are encoded to length <seq_len>; shorter inputs are zero-padded\n",
    "    and longer inputs are truncated.\n",
    "\n",
    "    Params:\n",
    "        tokenizer:             Tokenizer to use for encoding\n",
    "        questions (list[str]): Set of questions\n",
    "        contexts  (list[str]): Set of contexts (length must match questions)\n",
    "        seq_len (int):         Fixed length of encoded samples\n",
    "\n",
    "    Returns: Dictionary of [batch_size x seq_len] tensors (np.array) for\n",
    "        token IDs, segment IDs, and attention masks.\n",
    "    '''\n",
    "\n",
    "    input_ids = []\n",
    "    tkn_types = []\n",
    "    attn_mask = []\n",
    "\n",
    "    for q,c in zip(questions,contexts):\n",
    "        inputs = tokenizer.encode_plus(q,c,return_tensors='np',truncation=True,padding='max_length',max_length=seq_len)\n",
    "        input_ids.append(inputs['input_ids'])\n",
    "        tkn_types.append(inputs['token_type_ids'])\n",
    "        attn_mask.append(inputs['attention_mask'])\n",
    "\n",
    "    input_ids = np.vstack(input_ids)\n",
    "    tkn_types = np.vstack(tkn_types)\n",
    "    attn_mask = np.vstack(attn_mask)\n",
    "\n",
    "    return {\n",
    "        'input_ids' : input_ids,\n",
    "        'token_type_ids' : tkn_types,\n",
    "        'attention_mask' : attn_mask\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d8c89c-86ef-4f19-b8e2-a729b754b492",
   "metadata": {},
   "source": [
    "#### Initialize the Profiling Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94cdbfa7-9334-4e32-88d1-9025fe0fdca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 1\n",
    "batch_size = 2\n",
    "sequence_length = 384"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47d8238-e509-4173-b362-28e901664436",
   "metadata": {},
   "source": [
    "#### Download Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0c5b727-a0aa-49f3-bfde-506052b20efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model parameters...\n"
     ]
    }
   ],
   "source": [
    "compile_dir = f'compiled_tiny_bert'\n",
    "onnx_file = compile_dir + '/model.onnx'\n",
    "\n",
    "os.makedirs(compile_dir,exist_ok=True)\n",
    "\n",
    "\n",
    "hf_model_name = 'mrm8488/bert-tiny-finetuned-squadv2'\n",
    "\n",
    "print('Downloading model parameters...')\n",
    "model     = BertForQuestionAnswering.from_pretrained(hf_model_name).eval()\n",
    "tokenizer = BertTokenizer.from_pretrained(hf_model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2713296-94c3-46d9-a682-1dec525ff679",
   "metadata": {},
   "source": [
    "#### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e574ddc-97fa-423d-a767-585090fe0974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad_v2 (/home/auro/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading SQUADv2 dataset...\n"
     ]
    }
   ],
   "source": [
    "# Download dataset from HuggingFace hub\n",
    "print('Downloading SQUADv2 dataset...')\n",
    "squad = datasets.load_dataset('squad_v2', split='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f734af-7624-4113-9924-806cbdd3d3ba",
   "metadata": {},
   "source": [
    "#### Encode Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ce4a54f-111b-4db2-8098-80c1b2d4f87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding inputs...\n"
     ]
    }
   ],
   "source": [
    "# Encode plain-text paragraphs & questions into token IDs, segment IDs, and attention masks\n",
    "batches = []\n",
    "print('Encoding inputs...')\n",
    "\n",
    "for i in range(num_batches):\n",
    "    batch = list(squad)[i*batch_size:(i+1)*batch_size]\n",
    "    questions = [q['question'] for q in batch]\n",
    "    contexts  = [q['context']  for q in batch]\n",
    "    encoded_inputs = encode_batch(tokenizer,questions,contexts,sequence_length)\n",
    "    batches.append(encoded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1bf646-0f16-4ce8-9048-9ef5b84c52d7",
   "metadata": {},
   "source": [
    "#### Export the Model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b48414-e004-44ee-9a85-ad7ced37ad2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting model to ONNX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.8/site-packages/torch/onnx/utils.py:90: UserWarning: 'enable_onnx_checker' is deprecated and ignored. It will be removed in the next PyTorch release. To proceed despite ONNX checker failures, catch torch.onnx.ONNXCheckerError.\n",
      "  warnings.warn(\"'enable_onnx_checker' is deprecated and ignored. It will be removed in \"\n",
      "/opt/venv/lib/python3.8/site-packages/torch/onnx/utils.py:103: UserWarning: `use_external_data_format' is deprecated and ignored. Will be removed in next PyTorch release. The code will work as it is False if models are not larger than 2GB, Otherwise set to False because of size limits imposed by Protocol Buffers.\n",
      "  warnings.warn(\"`use_external_data_format' is deprecated and ignored. Will be removed in next \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['input_ids', 'attention_mask', 'token_type_ids'],\n",
       " ['start_logits', 'end_logits'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Exporting model to ONNX...')\n",
    "config = BertOnnxConfig(model.config, task='question-answering')\n",
    "export(tokenizer,model,config,opset=12,output=Path(onnx_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f1bfa2-158f-4f27-a256-38280e32bb21",
   "metadata": {},
   "source": [
    "#### Validate Model\n",
    "\n",
    "The model needs to get validated for Operator Coverage. Here the ONNX model is scanned and you get an output that shows a list of supported and unsupported operations by the compiler.\n",
    "\n",
    "The ONNX file path that is being validated is: `compiled_tiny_bert/model.onnx`\n",
    "\n",
    "The ``idiom.cc.onnx.check_op_cov`` API command invokes the Operator Coverage functionality. Here, it accepts two arguments: an ONNX model, and a .json file that defines the ONNX inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82695f6d-35c9-424f-865d-9c12afd59ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-18 19:05:17,623 - check-op-cov - INFO - check-op-cov v0.5.0\n",
      "Date and time: August 18, 2022 19:05:17\n",
      "Source model path: /idiom-eap/examples/00-getting-started/tutorials/bert-tiny/perf/compiled_tiny_bert/model.onnx\n",
      "2022-08-18 19:05:17,624 - check-op-cov - INFO - Output files will be saved in /idiom-eap/examples/00-getting-started/tutorials/bert-tiny/perf/compiled_tiny_bert/model_opcov\n",
      "2022-08-18 19:05:17,625 - check-op-cov - INFO - Running operator coverage tool...\n",
      "2022-08-18 19:05:17,781 - check-op-cov - INFO - Finished running operator coverage tool.\n",
      "2022-08-18 19:05:17,790 - check-op-cov - INFO - General messages from the compiler:\n",
      "ONNX opset version 12\n",
      "Setting parameter 'batch' to 1 from input declaration.\n",
      "Setting parameter 'sequence' to 384 from input declaration.\n",
      "ONNX IR version 7\n",
      "ONNX producer \"pytorch\" version 1.10\n",
      "ONNX model version 0\n",
      "\n",
      "2022-08-18 19:05:17,791 - check-op-cov - INFO - 224/224 operators from 21 op types passed. All ops are supported!\n",
      "2022-08-18 19:05:17,792 - check-op-cov - INFO - Op types summary:\n",
      "Op Type     Total          Supported      Unsupported    Failed         \n",
      "------------------------------------------------------------------------\n",
      "Add         34             34             0              0              \n",
      "Cast        1              1              0              0              \n",
      "Concat      8              8              0              0              \n",
      "Constant    39             39             0              0              \n",
      "Div         9              9              0              0              \n",
      "Erf         2              2              0              0              \n",
      "Gather      20             20             0              0              \n",
      "MatMul      17             17             0              0              \n",
      "Mul         10             10             0              0              \n",
      "Pow         5              5              0              0              \n",
      "ReduceMean  10             10             0              0              \n",
      "Reshape     8              8              0              0              \n",
      "Shape       17             17             0              0              \n",
      "Slice       1              1              0              0              \n",
      "Softmax     2              2              0              0              \n",
      "Split       1              1              0              0              \n",
      "Sqrt        5              5              0              0              \n",
      "Squeeze     2              2              0              0              \n",
      "Sub         6              6              0              0              \n",
      "Transpose   8              8              0              0              \n",
      "Unsqueeze   19             19             0              0              \n",
      "\n",
      "2022-08-18 19:05:17,793 - check-op-cov - INFO - Visuals not created because all ops are supported.\n"
     ]
    }
   ],
   "source": [
    "from idiom.cc.onnx import check_op_cov\n",
    "check_op_cov('compiled_tiny_bert/model.onnx', onnx_define_inputs=\"bert-inputs.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb534297-1425-4a8a-bd22-e47dc5f1cdd9",
   "metadata": {},
   "source": [
    "#### Compile\n",
    "\n",
    "The ``idiom.cc.onnx.compile`` API command invokes the Idiom Compiler, where an ONNX model is compiled for Envise. It accepts mainly two arguments:\n",
    "\n",
    "* **output_directory** \n",
    "\n",
    "    Directory where the output files will get stored after compilation.\n",
    "\n",
    "* **onnx_file_path**\n",
    "    \n",
    "    Path to the ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17926dd4-cf13-4803-a8f6-9a5492bca0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting compiling...\n",
      "Done compiling\n"
     ]
    }
   ],
   "source": [
    "compile_flags = [\n",
    "    f'--onnx-declare-input=input_ids[{batch_size},{sequence_length}]'\n",
    "]\n",
    "\n",
    "from idiom.cc.onnx import compile\n",
    "print('Starting compiling...')\n",
    "idiom.cc.onnx.compile(compile_dir, onnx_file, batch_size, compile_flags)\n",
    "print('Done compiling')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17071c66-bb37-4f4a-939f-55cbb74e6531",
   "metadata": {},
   "source": [
    "#### Profile Model\n",
    "\n",
    "The ``idiom.runtime.profile`` API command invokes the profiler. It measures the modelâ€™s performance metrics such as Inferences Per Second (IPS) and latency of your model for Envise by profiling the execution of the model at runtime. \n",
    "\n",
    "It accepts three arguments:\n",
    "\n",
    "* **Compiled Model Directory**\n",
    "\n",
    "    The Compiled Model Directory where the compilation output resides.\n",
    "\n",
    "* **Input data**\n",
    "\n",
    "    A sequence of dictionaries containing model inputs. \n",
    "\n",
    "* **Batch size**\n",
    "\n",
    "    The number of samples within a batch. This value is used to compute performance metrics.\n",
    "    \n",
    "    Note that we are setting the batch size to **1** in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fa26fce-3189-4b5a-bcb2-538cdf29a855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling inferencing...\n",
      "Profiling compiled_tiny_bert\n",
      "    Loading model\n",
      "    Profiling model execution\n",
      "        Running batch 1 of 1\n",
      "\n",
      "Performance Report\n",
      "\n",
      "Source model path: compiled_tiny_bert\n",
      "Batch size: 2\n",
      "Number of Envises: 0.5\n",
      "\n",
      "+------------------------------------+-------------------------+----------------------+\n",
      "|         Measurement Scope          |   Inferences per Second |   Batch Latency (ms) |\n",
      "+====================================+=========================+======================+\n",
      "|  System Performance (CPU Compute,  |                      32 |                63.42 |\n",
      "| Envise Compute, and Data Transfer) |                         |                      |\n",
      "+------------------------------------+-------------------------+----------------------+\n",
      "|  Envise Compute and Data Transfer  |                    5136 |                 0.39 |\n",
      "+------------------------------------+-------------------------+----------------------+\n",
      "|        Only Envise Compute         |                    7543 |                 0.27 |\n",
      "+------------------------------------+-------------------------+----------------------+\n",
      "|          Only CPU Compute          |                      32 |                63.04 |\n",
      "+------------------------------------+-------------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "import idiom.runtime\n",
    "print('Profiling inferencing...')\n",
    "idiom.runtime.profile(compile_dir, batches, detailed_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1a7484-6e31-40ac-8277-80391c72911d",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "This tutorial shows how to validate, compile, and profile a ``Bert-Tiny`` model, and measure its performance metrics for Envise-behavior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
